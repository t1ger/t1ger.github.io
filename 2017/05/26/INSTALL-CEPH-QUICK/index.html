	<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  <meta name="google-site-verification" content="B6QNJ8N2Q7hbnMncyrM5GMCWgK-vIAEx0o2DQre64ls">
  
  <title>INSTALL CEPH (QUICK) | t1ger的茶馆</title>
  <meta name="author" content="t1ger">
  
  <meta name="description" content="https://t1ger.github.io/">
  

  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta property="og:title" content="INSTALL CEPH (QUICK)">
  <meta property="og:site_name" content="t1ger的茶馆">

  
  
		<!-- favicon -->
		<link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
		<link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
		<link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
		<link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
		<link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
		<link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
		<link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
		<link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
		<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
		<link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
		<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
		<link rel="manifest" href="/manifest.json">
		<meta name="msapplication-TileColor" content="#009688">
		<meta name="msapplication-TileImage" content="/mstile-144x144.png">
		<meta name="theme-color" content="#009688">
		<!-- favicon end -->
    <!-- <link href="/favicon.ico" rel="icon"> -->
  

  <!-- toc -->
  <link rel="stylesheet" href="/libs/tocify/jquery.tocify.css" media="screen" type="text/css">

  <!-- <link rel="stylesheet" href="/libs/bs/css/bootstrap.min.css" media="screen" type="text/css"> -->
  <link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap/3.3.4/css/bootstrap.min.css" media="screen" type="text/css">

  <!-- material design -->
	<!-- <link rel="stylesheet" href="/libs/bs-material/css/ripples.min.css" media="screen" type="text/css"> -->
  <link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap-material/0.3.0/css/ripples.min.css" media="screen" type="text/css">
  <!-- <link rel="stylesheet" href="/libs/bs-material/css/material.min.css" media="screen" type="text/css"> -->
	<link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap-material/0.3.0/css/material.min.css" media="screen" type="text/css">

  <link rel="stylesheet" href="/css/highlight.light.css" media="screen" type="text/css">

  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">

  

  

  <script src="//apps.bdimg.com/libs/jquery/2.0.3/jquery.min.js"></script>
	<script>window.jQuery || document.write('<script src="/libs/jquery-2.0.3.min.js" type="text/javascript"><\/script>')</script>

</head>
</html>
 	<body>
	  <nav class="navbar navbar-default">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">菜单</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">t1ger的茶馆</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
            <ul class="nav navbar-nav navbar-right">
                
                <li>
                    <a href="/" title="">
                    <i class="fa fa-home"></i>首页
                    </a>
                </li>
                
                <li>
                    <a href="/archives" title="">
                    <i class="fa fa-list"></i>存档
                    </a>
                </li>
                
                <li>
                    <a href="/about" title="">
                    <i class="fa fa-info-circle"></i>关于
                    </a>
                </li>
                
                <li>
                    <a href="/atom.xml" title="这是一个订阅源">
                    <i class="fa fa-rss"></i>RSS
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>

	  <div class="container" >
	    <div class="row">
	
	<div class="col-md-9 center-content">
	

		<div class="content">
			<!-- index -->
		   

			  		<h2>INSTALL CEPH (QUICK)</h2>
					
					<div>
						<span class="post-time">2017-05-26 16:25:56</span>
					</div>	
					

					<div class="article-content">
						<h5 id="概念介绍"><a href="#概念介绍" class="headerlink" title="概念介绍"></a><b>概念介绍</b></h5><p>Ceph的部署模式下主要包含以下几个类型的节点:</p>
<ul>
<li>Ceph OSD: 主要用来存储数据，处理数据的replication,恢复，填充，调整资源组合以及通过检查其他OSD进程的心跳信息提供一些监控信息给Ceph Monitors . 当Ceph Storage Cluster 要准备2份数据备份时，要求至少有2个Ceph OSD进程的状态是active+clean状态</li>
<li>Monitor: 维护集群map的状态，主要包括monitor map, OSD map, Placement Group (PG) map, 以及CRUSH map. Ceph 维护了 Ceph Monitors, Ceph OSD Daemons, 以及PGs状态变化的历史记录 (called an “epoch”)</li>
<li>MDS: Ceph Metadata Server (MDS)存储的元数据代表Ceph的文件系统 (i.e., Ceph Block Devices 以及Ceph Object Storage 不适用 MDS). Ceph Metadata Servers 让系统用户可以执行一些POSIX文件系统的基本命令，例如ls,find 等</li>
<li>PG全称Placement Grouops，是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据</li>
<li>RBD全称RADOS block device，是Ceph对外提供的块设备服务</li>
<li>RGW全称RADOS gateway，是Ceph对外提供的对象存储服务，接口与S3和Swift兼容</li>
<li>RADOS全称Reliable Autonomic Distributed Object Store，是Ceph集群的精华，用户实现数据分配、Failover等集群操作</li>
<li>Librados是Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持</li>
<li>CephFS全称Ceph File System，是Ceph对外提供的文件系统服务</li>
<li>CRUSH是Ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方</li>
</ul>
<h5 id="部署方案"><a href="#部署方案" class="headerlink" title="部署方案"></a><b>部署方案</b></h5><p>通过ceph-deploy可以快速便捷的安装上ceph,此方法部署需要一个管理节点（admin node)和3个节点来做ceph的存储集群<br>三台装有CentOS 7的主机，每台主机有三个磁盘(虚拟机磁盘要大于100G),详细信息如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# cat /etc/redhat-release </span><br><span class="line">CentOS Linux release 7.3.1611 (Core) </span><br><span class="line"></span><br><span class="line">[root@node1 ~]# lsblk</span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0  100G  0 disk </span><br><span class="line">├─sda1            8:1    0  500M  0 part /boot</span><br><span class="line">└─sda2            8:2    0 99.5G  0 part </span><br><span class="line">  ├─system-root 253:0    0 95.6G  0 lvm  /</span><br><span class="line">  └─system-swap 253:1    0  3.9G  0 lvm  [SWAP]</span><br><span class="line">sdb               8:16   0    2T  0 disk </span><br><span class="line">sdc               8:32   0    2T  0 disk </span><br><span class="line">sdd               8:48   0    2T  0 disk </span><br><span class="line"></span><br><span class="line">[root@node1 ~]# cat /etc/hosts</span><br><span class="line">192.168.138.140 admin</span><br><span class="line">192.168.138.141 node1</span><br><span class="line">192.168.138.142 node2</span><br><span class="line">192.168.138.143 node3</span><br></pre></td></tr></table></figure>
<p>集群配置如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">admin	192.168.138.140  deploy</span><br><span class="line">node1   192.168.138.141  mon*1 osd*3</span><br><span class="line">node2   192.168.138.142  mon*1 osd*3</span><br><span class="line">node3   192.168.138.143  mon*1 osd*3</span><br></pre></td></tr></table></figure></p>
<p>备注:在生产环境,每个osd对应一块硬盘,每个osd需要一个journal,建议使用ssd作为osd硬盘的journal.<br>这里测试环境将采用data和journal在一个硬盘的做法</p>
<h5 id="预检Preflight"><a href="#预检Preflight" class="headerlink" title="预检Preflight"></a><b>预检Preflight</b></h5><p>在admin node 节点<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">1. install epel</span><br><span class="line">sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br><span class="line">2. add ceph-deploy source</span><br><span class="line">sudo cat &gt; /etc/yum.repos.d/ceph-deploy.repo &lt;&lt; EOF</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=https://download.ceph.com/rpm-jewel/el7/noarch/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo cat &gt; /etc/yum.repos.d/ceph.repo &lt;&lt; EOF</span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://download.ceph.com/rpm-jewel/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://download.ceph.com/rpm-jewel/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://download.ceph.com/rpm-jewel/el7/SRPMS</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">3. install ceph-deploy</span><br><span class="line">sudo yum update &amp;&amp; sudo yum install ceph-deploy -y</span><br></pre></td></tr></table></figure></p>
<p>在admin node上创建一个拥有sudo权限的用户，并有sudo权限,不要使用ceph这个用户名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo useradd -g wheel  neo &amp;&amp; su - neo</span><br><span class="line"></span><br><span class="line">ssh-keygen</span><br><span class="line">ssh-copy-id neo@node1</span><br><span class="line">ssh-copy-id neo@node2</span><br><span class="line">ssh-copy-id neo@node3</span><br></pre></td></tr></table></figure></p>
<p>如果开启了防火墙需要<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo firewall-cmd --zone=public --add-service=ceph-mon --permanent</span><br><span class="line">sudo firewall-cmd --zone=public --add-service=ceph --permanent</span><br><span class="line">sudo firewalld-cmd --reload</span><br><span class="line">或者</span><br><span class="line"># firewall-cmd --zone=public --add-port=6789/tcp --permanent</span><br><span class="line"># firewall-cmd --zone=public --add-port=6800-7100/tcp --permanent</span><br><span class="line"># firewall-cmd --reload</span><br></pre></td></tr></table></figure></p>
<p>For iptables, add port 6789 for Ceph Monitors and ports 6800:7300 for Ceph OSDs</p>
<h5 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a><b>开始安装</b></h5><p>1.建立部署目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[neo@admin ~]$ ceph-deploy --version</span><br><span class="line">1.5.37</span><br><span class="line">[neo@admin ~]$ mkdir cluster</span><br><span class="line">[neo@admin ~]$ cd cluster/</span><br></pre></td></tr></table></figure></p>
<p>2.清理旧配置,全新安装略过<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy purge node1 node2 node3</span><br><span class="line">ceph-deploy purgedata node1 node2 node3</span><br><span class="line">ceph-deploy forgetkeys</span><br></pre></td></tr></table></figure></p>
<p>3.Create the cluster<br>初始化集群，告诉 ceph-deploy 哪些节点是监控节点，命令成功执行后会在 cluster 目录下生成 ceph.conf, ceph.log, ceph.mon.keyring 等相关文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[neo@admin cluster]$ ceph-deploy new node1 node2 node3</span><br></pre></td></tr></table></figure></p>
<p>4.在每个 Ceph 节点上都安装 Ceph：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[neo@admin cluster]$ ceph-deploy install node1 node2 node3</span><br><span class="line">或者登陆节点,手动安装</span><br><span class="line">yum -y install ceph ceph-radosgw</span><br></pre></td></tr></table></figure></p>
<p>5.初始化监控节点：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[neo@admin cluster]$ ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure></p>
<p>6.查看一下 Ceph 存储节点的硬盘情况<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[neo@admin cluster]$ ceph-deploy disk list node1</span><br><span class="line">[neo@admin cluster]$ ceph-deploy disk list node2</span><br><span class="line">[neo@admin cluster]$ ceph-deploy disk list node3</span><br></pre></td></tr></table></figure></p>
<p>7.初始化 Ceph 硬盘，然后创建 osd 存储节点,(存储节点:单个硬盘:对应的 journal 分区，一一对应)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy disk zap node1:sdb node1:sdc node1:sdd </span><br><span class="line">ceph-deploy osd create node1:sdb node1:sdc node1:sdd </span><br><span class="line"></span><br><span class="line">ceph-deploy disk zap node2:sdb node2:sdc node2:sdd </span><br><span class="line">ceph-deploy osd create node2:sdb node2:sdc node2:sdd </span><br><span class="line"></span><br><span class="line">ceph-deploy disk zap node3:sdb node3:sdc node3:sdd</span><br><span class="line">ceph-deploy osd create node3:sdb node3:sdc node3:sdd</span><br></pre></td></tr></table></figure></p>
<p>8.最后，我们把生成的配置文件从 ceph-adm 同步部署到其他几个节点，使得每个节点的 ceph 配置一致：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[neo@admin cluster]$ ceph-deploy --overwrite-conf admin admin node1 node2 node3</span><br></pre></td></tr></table></figure>
<h5 id="测试"><a href="#测试" class="headerlink" title="测试"></a><b>测试</b></h5><p>看一下配置成功了没？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@admin ~]# ceph -s</span><br><span class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            too few PGs per OSD (21 &lt; min 30)</span><br><span class="line">     monmap e1: 3 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0,node3=192.168.138.143:6789/0&#125;</span><br><span class="line">            election epoch 6, quorum 0,1,2 node1,node2,node3</span><br><span class="line">     osdmap e53: 9 osds: 9 up, 9 in</span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v148: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">            307 MB used, 18377 GB / 18378 GB avail</span><br><span class="line">                  64 active+clean</span><br><span class="line">[root@admin ~]# ceph health</span><br><span class="line">HEALTH_WARN too few PGs per OSD (21 &lt; min 30)</span><br></pre></td></tr></table></figure></p>
<p>备注: 这里需要注意的事执行命令是在root权限下执行的，<br>因为etc/ceph/ceph.client.admin.keyring权限只允许root读写导致，亦可以给文件加权限解决<br>如果在neo权限下,报错如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[neo@admin cluster]$ ceph health</span><br><span class="line">2017-05-27 16:58:23.374074 7f567813a700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin: (2) No such file or directory</span><br><span class="line">2017-05-27 16:58:23.374109 7f567813a700 -1 monclient(hunting): ERROR: missing keyring, cannot use cephx for authentication</span><br><span class="line">2017-05-27 16:58:23.374113 7f567813a700  0 librados: client.admin initialization error (2) No such file or directory</span><br><span class="line">Error connecting to cluster: ObjectNotFound</span><br></pre></td></tr></table></figure></p>
<p>在我们执行ceph -s 时警告PG太少,我们接下来解决,由于是新配置的集群，只有一个pool</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[neo@admin ~]$ sudo ceph osd lspools</span><br><span class="line">0 rbd,</span><br></pre></td></tr></table></figure>
<p>查看rbd pool的PGS<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[neo@admin ~]$ sudo ceph osd pool get rbd pg_num</span><br><span class="line">pg_num: 64</span><br></pre></td></tr></table></figure></p>
<p>pgs为64，因为是3副本的配置，所以当有9个osd的时候, 每个osd上均分了64/9<em>3=21个pgs,也就是出现了如上的错误 小于最小配置30个<br>根据<a href="http://docs.ceph.com/docs/master/rados/configuration/pool-pg-config-ref/" target="_blank" rel="noopener">官方推荐</a>,Total PGs = (#OSDs </em> 100) / pool size 公式来决定 pg_num（pgp_num 应该设成和 pg_num 一样）,我们可以算出 100*9/3=300,Ceph推荐取最接近2的指数倍，所以选择256<br>解决办法：修改默认pool rbd的pgs和pgp_num,默认两个pg_num和pgp_num一样大小均为64，此处也将两个的值都设为256<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[neo@admin ~]$ sudo ceph osd pool set rbd pg_num 256</span><br><span class="line">set pool 0 pg_num to 256</span><br><span class="line"></span><br><span class="line">[neo@admin ~]$ sudo ceph osd pool set rbd pgp_num 256</span><br><span class="line">set pool 0 pgp_num to 128set pool 0 pgp_num to 256</span><br><span class="line"></span><br><span class="line">[neo@admin ~]$ sudo ceph health</span><br><span class="line">HEALTH_OK</span><br><span class="line"></span><br><span class="line">[neo@admin ~]$ sudo ceph -s</span><br><span class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: 3 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0,node3=192.168.138.143:6789/0&#125;</span><br><span class="line">            election epoch 18, quorum 0,1,2 node1,node2,node3</span><br><span class="line">     osdmap e116: 9 osds: 9 up, 9 in</span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v412: 256 pgs, 1 pools, 4 bytes data, 1 objects</span><br><span class="line">            324 MB used, 18377 GB / 18378 GB avail</span><br><span class="line">                 256 active+clean</span><br></pre></td></tr></table></figure></p>
<p>ref<br><a href="http://www.xuxiaopang.com/2016/10/10/ceph-full-install-el7-jewel/" target="_blank" rel="noopener">CEPH部署完整版(el7+jewel)</a><br><a href="http://ceph.com/install/" target="_blank" rel="noopener">INSTALL CEPH (QUICK)</a><br><a href="http://www.codexiu.cn/linux/blog/42056/" target="_blank" rel="noopener">centos7 ceph-deploy 安装jewel</a><br><a href="http://zedshadow.leanote.com/post/%E4%BD%BF%E7%94%A8ceph-deploy%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2Ceph" target="_blank" rel="noopener">centos7使用ceph-deploy快速部署Kraken版Ceph</a><br><a href="http://www.vpsee.com/2015/07/install-ceph-on-centos-7/" target="_blank" rel="noopener">在 CentOS 7.1 上安装分布式存储系统 Ceph</a><br><a href="http://www.cnblogs.com/zhangzhengyan/p/5839897.html" target="_blank" rel="noopener">ceph 创建和删除osd</a><br><a href="http://blog.csdn.net/xiongwenwu/article/details/53120415" target="_blank" rel="noopener">三种增删osd的方法数据量迁移大小</a><br><a href="https://www.virtualtothecore.com/en/quickly-build-a-new-ceph-cluster-with-ceph-deploy-on-centos-7/" target="_blank" rel="noopener">Quickly build a new Ceph cluster with ceph-deploy on CentOS 7</a><br><a href="http://blog.csdn.net/chinagissoft/article/details/50491429" target="_blank" rel="noopener">Ceph 集群部署</a><br><a href="https://my.oschina.net/diluga/blog/528618" target="_blank" rel="noopener">PG设置与规划</a><br><a href="https://my.oschina.net/xiaozhublog/blog/664560" target="_blank" rel="noopener">health HEALTH_WARN too few PGs per OSD</a><br><a href="https://github.com/thesues/cephdoc/blob/master/ceph-deploy-cn.markdown" target="_blank" rel="noopener">thesues/cephdoc</a><br><a href="http://www.isjian.com/ceph/deploy-a-ceph-cluster-manually/" target="_blank" rel="noopener">手工部署一个ceph集群</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>

							<!-- 支付宝打赏图案 -->
      						<!--
      						<div id="donate_guide" class="donate_bar center ">
          					<a href="http://mypay-1253516637.costj.myqcloud.com/2000.jpg" title="支付宝打赏" class="fancybox" rel="article0"       style="float:left;margin-left:25%;margin-right:2px;">
          					<img src="http://mypay-1253516637.costj.myqcloud.com/2000.jpg" title="支付宝打赏" height="164px" width="164px">
          					</a> 
      						</div>
      						-->
					</div>

			  <!-- about -->
			  
		</div>

		<!-- pagination -->
	  
	  
	    
		<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC8zOTQ5MS8xNjAxOA==">
	<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
	</script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->
		
<!--		<div class="comment-section">
  
  


</div> -->
	</div>

	

</div>


		<footer>
			


<p>
  由 <a href="https://hexo.io">hexo</a> 强力驱动 |&nbsp;&nbsp;本站总点击 <span id="busuanzi_value_site_pv"></span> 次
&nbsp;&nbsp;|&nbsp;&nbsp;您是第 <span id="busuanzi_value_site_uv"></span> 位访客
</p>
<p>
  &copy; 2025 <a href="https://t1ger.github.io"> t1ger </a>
  &nbsp;&nbsp;|&nbsp;&nbsp;<span><a href="/baidusitemap.xml">百度网站地图</a></span>

</p>

<script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<script>
(function(){
    var bp = document.createElement('script');
    bp.src = '//push.zhanzhang.baidu.com/push.js';
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

<a id="gotop" href="#" title="back to top"><i class="mdi-hardware-keyboard-arrow-up"></i></a>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-99921399-1', 'auto');
  ga('send', 'pageview');

</script>
		</footer>
	  </div>

		<!-- <script src="/libs/bs/js/bootstrap.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap/3.3.4/js/bootstrap.min.js"></script>
		<script>(typeof $().modal == 'function')|| document.write('<script src="/libs/bs/js/bootstrap.min.js" type="text/javascript"><\/script>')</script>

		<!-- material design -->
		<!-- <script src="/libs/bs-material/js/ripples.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap-material/0.3.0/js/ripples.min.js"></script>
		<!-- <script src="/libs/bs-material/js/material.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap-material/0.3.0/js/material.min.js"></script>
		<!-- toc -->
		<!-- <script src="/libs/tocify/jquery-ui.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/jqueryui/1.10.4/jquery-ui.min.js"></script>
		<script src="/libs/tocify/jquery.tocify.custom.js"></script>

		<script src="/js/main.js"></script>

	</body>
</html>
