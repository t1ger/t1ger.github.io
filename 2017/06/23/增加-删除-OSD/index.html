	<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  <meta name="google-site-verification" content="B6QNJ8N2Q7hbnMncyrM5GMCWgK-vIAEx0o2DQre64ls">
  
  <title>增加/删除 OSD | t1ger的茶馆</title>
  <meta name="author" content="t1ger">
  
  <meta name="description" content="https://t1ger.github.io/">
  

  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta property="og:title" content="增加/删除 OSD">
  <meta property="og:site_name" content="t1ger的茶馆">

  
  
		<!-- favicon -->
		<link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
		<link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
		<link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
		<link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
		<link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
		<link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
		<link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
		<link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
		<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
		<link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
		<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
		<link rel="manifest" href="/manifest.json">
		<meta name="msapplication-TileColor" content="#009688">
		<meta name="msapplication-TileImage" content="/mstile-144x144.png">
		<meta name="theme-color" content="#009688">
		<!-- favicon end -->
    <!-- <link href="/favicon.ico" rel="icon"> -->
  

  <!-- toc -->
  <link rel="stylesheet" href="/libs/tocify/jquery.tocify.css" media="screen" type="text/css">

  <!-- <link rel="stylesheet" href="/libs/bs/css/bootstrap.min.css" media="screen" type="text/css"> -->
  <link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap/3.3.4/css/bootstrap.min.css" media="screen" type="text/css">

  <!-- material design -->
	<!-- <link rel="stylesheet" href="/libs/bs-material/css/ripples.min.css" media="screen" type="text/css"> -->
  <link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap-material/0.3.0/css/ripples.min.css" media="screen" type="text/css">
  <!-- <link rel="stylesheet" href="/libs/bs-material/css/material.min.css" media="screen" type="text/css"> -->
	<link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap-material/0.3.0/css/material.min.css" media="screen" type="text/css">

  <link rel="stylesheet" href="/css/highlight.light.css" media="screen" type="text/css">

  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">

  

  

  <script src="//apps.bdimg.com/libs/jquery/2.0.3/jquery.min.js"></script>
	<script>window.jQuery || document.write('<script src="/libs/jquery-2.0.3.min.js" type="text/javascript"><\/script>')</script>

</head>
</html>
 	<body>
	  <nav class="navbar navbar-default">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">菜单</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">t1ger的茶馆</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
            <ul class="nav navbar-nav navbar-right">
                
                <li>
                    <a href="/" title="">
                    <i class="fa fa-home"></i>首页
                    </a>
                </li>
                
                <li>
                    <a href="/archives" title="">
                    <i class="fa fa-list"></i>存档
                    </a>
                </li>
                
                <li>
                    <a href="/about" title="">
                    <i class="fa fa-info-circle"></i>关于
                    </a>
                </li>
                
                <li>
                    <a href="/atom.xml" title="这是一个订阅源">
                    <i class="fa fa-rss"></i>RSS
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>

	  <div class="container" >
	    <div class="row">
	
	<div class="col-md-9 center-content">
	

		<div class="content">
			<!-- index -->
		   

			  		<h2>增加/删除 OSD</h2>
					
					<div>
						<span class="post-time">2017-06-23 18:51:21</span>
					</div>	
					

					<div class="article-content">
						<h5 id="增加-OSD-手动"><a href="#增加-OSD-手动" class="headerlink" title="增加 OSD(手动)"></a><b>增加 OSD(手动)</b></h5><p>首先修改各个节点的/etc/hosts信息,增加新节点信息,并添加ceph.client.admin.keyring,确保ceph.client.admin.keyring有正确的权限<br>要增加一个 OSD,要依次创建数据目录、把硬盘挂载到数据目录、把 OSD 加入集群、然后把它加入 CRUSH Map<br><b>备注</b>:Ceph 喜欢统一的硬件,与存储池无关。如果你要新增容量不一的硬盘驱动器,还需调整它们的权重。但是,为实现最佳性能，CRUSH 的分级结构最好按类型、容量来组织</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@node4 ~]# parted -s /dev/sdb mklabel gpt</span><br><span class="line">#删除所有分区</span><br><span class="line">[root@node4 ~]# sgdisk --zap-all --clear --mbrtogpt /dev/sdb</span><br><span class="line">GPT data structures destroyed! You may now partition the disk using fdisk or</span><br><span class="line">other utilities.</span><br><span class="line">The operation has completed successfully.</span><br><span class="line">#打印硬盘信息</span><br><span class="line">[root@node4 ~]# sgdisk -p /dev/sdb   </span><br><span class="line">[root@node4 ~]# ceph-disk prepare --cluster ceph --fs-type xfs /dev/sdb</span><br><span class="line">[root@node4 ~]# ceph-disk activate /dev/sdb</span><br><span class="line">备注:</span><br><span class="line">ceph-disk prepare --cluster  --cluster-uuid  --fs-type xfs|ext4|btrfs /device </span><br><span class="line">cluster-uuid ( b71a3eb1-e253-410a-bf11-84ae01bad654 )</span><br><span class="line">cluster name – default name is ceph unless specified otherwise when ran ceph-deploy</span><br><span class="line">eg ceph-deploy –cluster=cluster_name</span><br></pre></td></tr></table></figure>
<p>到这里我们就添加完了osd. 感兴趣的同学可以往下看,这里ceph-disk到底帮我们做了什么操作呢?</p>
<p>1.创建 OSD。如果未指定 UUID,OSD 启动时会自动生成一个。下列命令会输出 OSD 号,后续步骤你会用到<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph osd create [&#123;uuid&#125; [&#123;id&#125;]]</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# ceph osd create           </span><br><span class="line">9</span><br></pre></td></tr></table></figure></p>
<p>如果指定了可选参数 {id} ，那么它将作为 OSD id 。要注意，如果此数字已使用，此命令会出错。<br><b>建议</b>：一般来说,我们不建议指定 {id} 。因为 ID 是按照数组分配的,跳过一些依然会浪费内存；<br>尤其是跳过太多、或者集群很大时，会更明显。若未指定 {id} ,将用最小可用数字。<br>2.在新 OSD 主机上创建数据目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ssh &#123;new-osd-host&#125;</span><br><span class="line">sudo mkdir /var/lib/ceph/osd/ceph-&#123;osd-number&#125;</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# mkdir /var/lib/ceph/osd/ceph-9</span><br><span class="line">[root@node4 ~]# chown ceph:ceph -R /var/lib/ceph/osd/ceph-9</span><br></pre></td></tr></table></figure></p>
<p>3.建立分区,可以参考<a href="http://ceph.com/geen-categorie/creating-a-ceph-osd-from-a-designated-disk-partition/" target="_blank" rel="noopener">这里</a>,关于sgdisk的用法参考<a href="http://hustcat.github.io/sgdisk-basic" target="_blank" rel="noopener">这里</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@node4 ~]# sgdisk -n 1:10487808:4194301951 -t 1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -p /dev/sdb</span><br><span class="line">Disk /dev/sdb: 4194304000 sectors, 2.0 TiB</span><br><span class="line">Logical sector size: 512 bytes</span><br><span class="line">Disk identifier (GUID): A996FDA1-5621-45CD-871A-028E30E33027</span><br><span class="line">Partition table holds up to 128 entries</span><br><span class="line">First usable sector is 34, last usable sector is 4194303966</span><br><span class="line">Partitions will be aligned on 2048-sector boundaries</span><br><span class="line">Total free space is 10489789 sectors (5.0 GiB)</span><br><span class="line"></span><br><span class="line">Number  Start (sector)    End (sector)  Size       Code  Name</span><br><span class="line">   1        10487808      4194301951   1.9 TiB     FFFF  </span><br><span class="line">The operation has completed successfully.</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# sgdisk -n 2:2048:10487807 -t 2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -p /dev/sdb</span><br><span class="line">Disk /dev/sdb: 4194304000 sectors, 2.0 TiB</span><br><span class="line">Logical sector size: 512 bytes</span><br><span class="line">Disk identifier (GUID): A996FDA1-5621-45CD-871A-028E30E33027</span><br><span class="line">Partition table holds up to 128 entries</span><br><span class="line">First usable sector is 34, last usable sector is 4194303966</span><br><span class="line">Partitions will be aligned on 2048-sector boundaries</span><br><span class="line">Total free space is 4029 sectors (2.0 MiB)</span><br><span class="line"></span><br><span class="line">Number  Start (sector)    End (sector)  Size       Code  Name</span><br><span class="line">   1        10487808      4194301951   1.9 TiB     FFFF  </span><br><span class="line">   2            2048        10487807   5.0 GiB     FFFF  </span><br><span class="line">The operation has completed successfully.</span><br></pre></td></tr></table></figure></p>
<p>4.给数据盘和日志盘做标记,typecode参考<a href="https://github.com/ceph/ceph/blob/v0.67.4/src/ceph-disk#L65" target="_blank" rel="noopener">这里</a>,关于磁盘自动挂载的请参考<a href="http://www.zphj1987.com/2016/12/22/Ceph%E6%95%B0%E6%8D%AE%E7%9B%98%E6%80%8E%E6%A0%B7%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E6%8C%82%E8%BD%BD/" target="_blank" rel="noopener">这里</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">sgdisk 例子</span><br><span class="line">#分5120M大小</span><br><span class="line">/usr/sbin/sgdisk  --new=2:0:5120M --change-name=2:ceph-journal  --partition-guid=2:150f0081-c630-44c9-ad21-7d95613866ea  --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106    --mbrtogpt -- /dev/sdb</span><br><span class="line">#largest-new 将剩余block全部使用</span><br><span class="line">/usr/sbin/sgdisk --largest-new=1 --change-name=1:ceph-data --partition-guid=1:db182a1d-f8c6-4660-9c12-0222e2459dd5 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb</span><br><span class="line">#查看udev事件队列，如果所有的events已处理则退出</span><br><span class="line">/sbin/udevadm settle</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# /usr/sbin/sgdisk  --change-name=2:&apos;ceph journal&apos; --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106  -- /dev/sdb</span><br><span class="line">The operation has completed successfully.</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# /usr/sbin/sgdisk  --change-name=1:&apos;ceph data&apos; --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb</span><br><span class="line">Warning: The kernel is still using the old partition table.</span><br><span class="line">The new table will be used at the next reboot.</span><br><span class="line">The operation has completed successfully.</span><br><span class="line"></span><br><span class="line">[root@node4 ceph-9]# mkfs.xfs /dev/sdb1</span><br><span class="line">[root@node4 ceph-9]# mount /dev/sdb1 /var/lib/ceph/osd/ceph-9</span><br></pre></td></tr></table></figure></p>
<p>5.初始化 OSD 数据目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">ssh &#123;new-osd-host&#125;</span><br><span class="line">ceph-osd -i &#123;osd-num&#125; --mkfs --mkkey</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# ceph-osd -i 9 --mkfs --mkkey </span><br><span class="line">2017-06-29 16:23:05.161478 7f614226e800 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway</span><br><span class="line">2017-06-29 16:23:05.200065 7f614226e800 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway</span><br><span class="line">2017-06-29 16:23:05.221105 7f614226e800 -1 filestore(/var/lib/ceph/osd/ceph-9) could not find #-1:7b3f43c4:::osd_superblock:0# in index: (2) No such file or directory</span><br><span class="line">2017-06-29 16:23:05.237813 7f614226e800 -1 created object store /var/lib/ceph/osd/ceph-9 for osd.9 fsid d6d92de4-2a08-4bd6-a749-6c104c88fc40</span><br><span class="line">2017-06-29 16:23:05.237987 7f614226e800 -1 auth: error reading file: /var/lib/ceph/osd/ceph-9/keyring: can&apos;t open /var/lib/ceph/osd/ceph-9/keyring: (2) No such file or directory</span><br><span class="line">2017-06-29 16:23:05.238397 7f614226e800 -1 created new key in keyring /var/lib/ceph/osd/ceph-9/keyring</span><br><span class="line"></span><br><span class="line">[root@node4 ceph-9]# cd /var/lib/ceph/osd/ceph-9</span><br><span class="line">[root@node4 ceph-9]# rm -f journal</span><br><span class="line">[root@node4 ceph-9]# ll /dev/disk/by-partuuid/</span><br><span class="line">total 0</span><br><span class="line">lrwxrwxrwx 1 root root 10 Jun 30 11:35 c50d9217-c928-49a3-be1b-55990facf2e0 -&gt; ../../sdb2</span><br><span class="line">lrwxrwxrwx 1 root root 10 Jun 30 11:36 ced5d1bb-568c-4317-954f-efc63fa3bcaa -&gt; ../../sdb1</span><br><span class="line">[root@node4 ceph-9]# ln -s /dev/disk/by-partuuid/c50d9217-c928-49a3-be1b-55990facf2e0 journal</span><br><span class="line">[root@node4 ceph-9]# chown ceph:ceph -R /var/lib/ceph/osd/ceph-9</span><br><span class="line">[root@node4 ceph-9]# chown ceph:ceph /var/lib/ceph/osd/ceph-9/journal</span><br></pre></td></tr></table></figure></p>
<p>在启动 ceph-osd 前，数据目录必须是空的<br>6.注册 OSD 认证密钥,ceph-{osd-num} 路径里的 ceph 值应该是 $cluster-$id,如果你的集群名字不是 ceph,那就用自己集群的名字<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ceph auth add osd.&#123;osd-num&#125; osd &apos;allow *&apos; mon &apos;allow rwx&apos; -i /var/lib/ceph/osd/ceph-&#123;osd-num&#125;/keyring</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# ceph auth add osd.9 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-9/keyring</span><br><span class="line">added key for osd.9</span><br><span class="line"></span><br><span class="line">#创建client.bootstrap-osd key文件(本节点新建第一个osd时才需要)</span><br><span class="line">[root@node4 ~]# ceph auth get-or-create client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring</span><br><span class="line">[root@node4 ~]# chmod 600 /var/lib/ceph/bootstrap-osd/ceph.keyring</span><br><span class="line">[root@node4 ~]# chown ceph:ceph /var/lib/ceph/bootstrap-osd/ceph.keyring</span><br></pre></td></tr></table></figure></p>
<p>7.把新 OSD 加入 CRUSH Map 中,以便它可以开始接收数据。用 ceph osd crush add 命令把 OSD 加入 CRUSH 分级结构的合适位置。<br>如果你指定了不止一个 bucket，此命令会把它加入你所指定的 bucket 中最具体的一个，并且把此 bucket 挪到你指定的其它 bucket 之内<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ceph osd crush add &#123;id-or-name&#125; &#123;weight&#125; [&#123;bucket-type&#125;=&#123;bucket-name&#125; ...]</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# ceph osd crush add-bucket node4 host</span><br><span class="line">added bucket node4 type host to crush map</span><br><span class="line">[root@node4 ~]# ceph osd crush move node4 root=default</span><br><span class="line">moved item id -5 name &apos;node4&apos; to location &#123;root=default&#125; in crush map</span><br><span class="line">[root@node4 ~]# ceph osd crush add osd.9 1.0 host=node4</span><br><span class="line">add item id 9 name &apos;osd.9&apos; weight 1 at location &#123;host=node4&#125; to crush map</span><br></pre></td></tr></table></figure></p>
<p>你也可以反编译 CRUSH Map、把 OSD 加入设备列表、以 bucket 的形式加入主机（如果它没在 CRUSH Map 里）、以条目形式把设备加入主机、分配权重、重编译并应用它<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node4 ~]# ceph osd getcrushmap -o crushmap.txt</span><br><span class="line">[root@node4 ~]# crushtool -d crushmap.txt -o crushmap-decompile</span><br><span class="line">[root@node4 ~]# vim crushmap-decompile</span><br><span class="line">删除掉node4相关的信息</span><br><span class="line">[root@node4 ~]# crushtool -c crushmap-decompile  -o crushmap-compile</span><br><span class="line">[root@node4 ~]# ceph osd setcrushmap -i crushmap-compile </span><br><span class="line">set crush map</span><br></pre></td></tr></table></figure></p>
<p>8.启动 OSD。把 OSD 加入 Ceph 后， OSD 就在配置里了。然而它还没运行，它现在的状态为 down &amp; out 。你必须先启动 OSD 它才能收数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl start ceph-osd@&#123;osd-num&#125;</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# systemctl start ceph-osd@9</span><br></pre></td></tr></table></figure></p>
<p>启动了 OSD ，其状态就变成了 up &amp; in<br>遇到的问题:<br>[root@node4 ~]# systemctl start ceph-osd@9 启动不成功,报错如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@node4 ~]# /usr/bin/ceph-osd -f  --cluster ceph --id 9 --setuser ceph --setgroup ceph</span><br><span class="line">starting osd.9 at :/0 osd_data /var/lib/ceph/osd/ceph-9 /var/lib/ceph/osd/ceph-9/journal</span><br><span class="line">2017-06-30 11:49:18.599998 7f3f36afb800 -1 journal FileJournal::open: ondisk fsid 0c8bb770-f16a-4208-91d2-7e659768fbc8 doesn&apos;t match expected 03bba8cd-3765-4364-b727-e4f9269447cc, invalid (someone else&apos;s?) journal</span><br></pre></td></tr></table></figure></p>
<p>解决方法:Create new journal<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-osd --mkjournal -i &lt;osd num&gt;</span><br><span class="line"></span><br><span class="line">[root@node4 osd]# ceph-osd --mkjournal -i 9</span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  70 00 05 00 00 00 00 0a 00 00 00 00 20 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line">2017-06-30 11:59:44.328209 7fc180861800 -1 journal check: ondisk fsid 0c8bb770-f16a-4208-91d2-7e659768fbc8 doesn&apos;t match expected 03bba8cd-3765-4364-b727-e4f9269447cc, invalid (someone else&apos;s?) journal</span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  70 00 05 00 00 00 00 0a 00 00 00 00 20 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line">2017-06-30 11:59:44.338081 7fc180861800 -1 created new journal /var/lib/ceph/osd/ceph-9/journal for object store /var/lib/ceph/osd/ceph-9</span><br><span class="line"></span><br><span class="line">#启动成功</span><br><span class="line">systemctl start ceph-osd@9</span><br></pre></td></tr></table></figure></p>
<h5 id="增加-OSD-ceph-deploy"><a href="#增加-OSD-ceph-deploy" class="headerlink" title="增加 OSD(ceph-deploy)"></a><b>增加 OSD(ceph-deploy)</b></h5><p>1.登入 ceph-deploy 工具所在的 Ceph admin 节点，进入工作目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ssh &#123;ceph-deploy-node&#125;</span><br><span class="line">cd /path/ceph-deploy-work-path</span><br><span class="line"></span><br><span class="line">[neo@admin ~]$ cd cluster/</span><br></pre></td></tr></table></figure></p>
<p>2.列举磁盘。<br>执行下列命令列举一节点上的磁盘：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy disk list &#123;node-name [node-name]...&#125;</span><br><span class="line"></span><br><span class="line">[neo@admin cluster]$  ceph-deploy disk list node4</span><br></pre></td></tr></table></figure></p>
<p>3.格式化磁盘。<br>用下列命令格式化（删除分区表）磁盘，以用于 Ceph :<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy disk zap &#123;osd-server-name&#125;:&#123;disk-name&#125;</span><br><span class="line"></span><br><span class="line">[neo@admin cluster]$  ceph-deploy disk zap node4:sdb</span><br></pre></td></tr></table></figure></p>
<p><b>重要</b>： 这会删除磁盘上的所有数据<br>4.准备 OSD<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy osd prepare &#123;node-name&#125;:&#123;data-disk&#125;[:&#123;journal-disk&#125;]</span><br><span class="line"></span><br><span class="line">[neo@admin cluster]$ ceph-deploy osd prepare node4:sdb</span><br></pre></td></tr></table></figure></p>
<p>prepare 命令只准备 OSD 。在大多数操作系统中，硬盘分区创建后，不用 activate 命令也会自动执行 activate 阶段（通过 Ceph 的 udev 规则）<br>前例假定一个硬盘只会用于一个 OSD 守护进程，以及一个到 SSD 日志分区的路径。<br>我们建议把日志存储于另外的驱动器以最优化性能；你也可以指定一单独的驱动器用于日志（也许比较昂贵）、或者把日志放到 OSD 数据盘（不建议，因为它有损性能,这里放在一起了）。<br><b>注意</b>： 在一个节点运行多个 OSD 守护进程、且多个 OSD 守护进程共享一个日志分区时，你应该考虑整个节点的最小 CRUSH 故障域，<br>因为如果这个 SSD 坏了，所有用其做日志的 OSD 守护进程也会失效<br>5.准备好 OSD 后，可以用下列命令激活它<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy osd activate &#123;node-name&#125;:&#123;data-disk-partition&#125;[:&#123;journal-disk-partition&#125;]</span><br><span class="line">[neo@admin cluster]$  ceph-deploy osd activate node4:/dev/sdb1</span><br></pre></td></tr></table></figure></p>
<p>activate 命令会让 OSD 进入 up 且 in 状态。该命令使用的分区路径是前面 prepare 命令创建的</p>
<h5 id="删除-OSD-手动"><a href="#删除-OSD-手动" class="headerlink" title="删除 OSD(手动)"></a><b>删除 OSD(手动)</b></h5><p>在 Ceph 里，一个 OSD 通常是一台主机上的一个 ceph-osd 守护进程、它运行在一个硬盘之上。<br>如果一台主机上有多个数据盘，你得逐个删除其对应 ceph-osd。<br>通常，操作前应该检查集群容量，看是否快达到上限了，确保删除 OSD 后不会使集群达到 near full 比率<br><b>警告</b>：删除 OSD 时不要让集群达到 full ratio 值，删除 OSD 可能导致集群达到或超过 full ratio 值。</p>
<p>1.调整osd的crush weight<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh &#123;osd-host&#125;</span><br><span class="line">[root@node4 ~]# ceph osd crush reweight osd.10  0.1</span><br></pre></td></tr></table></figure></p>
<p>备注：可以分几次调整将crush 的weight 减低到0 ，目的就是让数据慢慢的分布到其他节点上，直到完全迁移到其他osd,<br>这个地方不光调整了osd 的crush weight ，实际上同时调整了host 的 weight ，这样会调整集群的整体的crush 分布，在osd 的crush 为0 后， 再对这个osd的任何删除相关操作都不会影响到集群的数据的分布</p>
<p>2.停止需要剔除的 OSD 进程，让其他的 OSD 知道这个 OSD 不提供服务了。停止 OSD 后，状态变为 down<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl stop  ceph-osd@id=&#123;osd-num&#125;</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# systemctl stop ceph-osd@10</span><br></pre></td></tr></table></figure></p>
<p>3.将 OSD 标记为 out 状态，这个一步是告诉 mon，这个 OSD 已经不能服务了，需要在其他的 OSD 上进行数据的均衡和恢复了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph osd out &#123;osd-num&#125;</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# ceph osd out 10</span><br><span class="line">marked out osd.10.</span><br></pre></td></tr></table></figure></p>
<p>执行完这一步后，会触发数据的恢复过程。此时应该等待数据恢复结束，集群恢复到 HEALTH_OK 状态，再进行下一步操作</p>
<p>4.删除 CRUSH Map 中的对应 OSD 条目，它就不再接收数据了。你也可以反编译 CRUSH Map、删除 device 列表条目、删除对应的 host 桶条目或删除 host 桶（如果它在 CRUSH Map 里，而且你想删除主机），重编译 CRUSH Map 并应用它<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph osd crush remove &#123;name&#125;</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# ceph osd crush remove osd.10</span><br><span class="line">removed item id 10 name &apos;osd.10&apos; from crush map</span><br></pre></td></tr></table></figure></p>
<p>该步骤会触发数据的重新分布。等待数据重新分布结束，整个集群会恢复到 HEALTH_OK 状态<br>5.删除 OSD 认证密钥：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph auth del osd.&#123;osd-num&#125;</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# ceph auth del osd.10</span><br><span class="line">updated</span><br></pre></td></tr></table></figure></p>
<p>6.删除 OSD<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph osd rm &#123;osd-num&#125;</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# ceph osd rm 10</span><br><span class="line">removed osd.10</span><br></pre></td></tr></table></figure></p>
<p>7.卸载 OSD 的挂载点.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo umount /var/lib/ceph/osd/$cluster-&#123;osd-num&#125;</span><br><span class="line"></span><br><span class="line">[root@node4 ~]# umount /var/lib/ceph/osd/ceph-10</span><br></pre></td></tr></table></figure></p>
<p>8.登录到保存 ceph.conf 主拷贝的主机<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh &#123;admin-host&#125;</span><br><span class="line">cd /etc/ceph</span><br><span class="line">vim ceph.conf</span><br></pre></td></tr></table></figure></p>
<p>9.从 ceph.conf 配置文件里删除对应条目<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[osd.10]</span><br><span class="line">        host = &#123;hostname&#125;</span><br></pre></td></tr></table></figure></p>
<p>10.从保存 ceph.conf 主拷贝的主机，把更新过的 ceph.conf 拷贝到集群其他主机的 /etc/ceph 目录下<br>如果在 ceph.conf 中没有定义各 OSD 入口，就不必执行第 8 ~ 10 步</p>
<h5 id="删除-OSD-ceph-deploy"><a href="#删除-OSD-ceph-deploy" class="headerlink" title="删除 OSD(ceph-deploy)"></a><b>删除 OSD(ceph-deploy)</b></h5><p>ref<br><a href="https://lihaijing.gitbooks.io/ceph-handbook/content/Operation/add_rm_osd.html" target="_blank" rel="noopener">增加/删除 OSD</a><br><a href="https://xiaoquqi.github.io/blog/2015/05/12/ceph-osd-is-full/" target="_blank" rel="noopener">Ceph集群磁盘没有剩余空间的解决方法</a><br><a href="https://github.com/chenzhongtao/work_summary/blob/master/ceph_doc/Ceph%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0ssd%E7%A3%81%E7%9B%98%E5%81%9Acache%20tier.txt" target="_blank" rel="noopener">Ceph如何添加ssd磁盘做cache tier.txt</a><br><a href="http://ceph.com/planet/%E8%AE%B0%E6%9C%80%E8%BF%91%E4%B8%80%E6%AC%A1ceph%E6%95%85%E9%9A%9C%E4%BF%AE%E5%A4%8D/" target="_blank" rel="noopener">记最近一次ceph故障修复</a><br><a href="https://forest.gitbooks.io/ceph-practice/content/troubleshoot.html" target="_blank" rel="noopener">故障定位和处理</a><br><a href="http://www.isjian.com/ceph/ceph-cluster-add-or-remove-osd-manual" target="_blank" rel="noopener">ceph集群中进行osd的手动添加移除</a><br><a href="http://ceph.com/geen-categorie/creating-a-ceph-osd-from-a-designated-disk-partition/" target="_blank" rel="noopener">Creating a Ceph OSD from a designated disk partition</a><br><a href="http://www.zphj1987.com/2015/11/12/%E5%A6%82%E4%BD%95%E5%88%A0%E9%99%A4%E4%B8%80%E5%8F%B0OSD%E4%B8%BB%E6%9C%BA/" target="_blank" rel="noopener">如何删除一台OSD主机</a><br><a href="https://ekuric.wordpress.com/2016/01/11/addremove-ceph-osd-object-storage-device/" target="_blank" rel="noopener">add/remove CEPH OSD – Object Storage Device</a><br><a href="https://www.xncoding.com/2017/03/14/ceph/disk-partition.html" target="_blank" rel="noopener">Linux磁盘分区总结</a><br><a href="http://bbs.ceph.org.cn/article/36" target="_blank" rel="noopener">Ceph：SSD日志故障后的OSD恢复</a><br><a href="https://lihaijing.gitbooks.io/ceph-handbook/content/Advance_usage/change_osd_journal.html" target="_blank" rel="noopener">更换 OSD Journal</a></p>

							<!-- 支付宝打赏图案 -->
      						<!--
      						<div id="donate_guide" class="donate_bar center ">
          					<a href="http://mypay-1253516637.costj.myqcloud.com/2000.jpg" title="支付宝打赏" class="fancybox" rel="article0"       style="float:left;margin-left:25%;margin-right:2px;">
          					<img src="http://mypay-1253516637.costj.myqcloud.com/2000.jpg" title="支付宝打赏" height="164px" width="164px">
          					</a> 
      						</div>
      						-->
					</div>

			  <!-- about -->
			  
		</div>

		<!-- pagination -->
	  
	  
	    
		<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC8zOTQ5MS8xNjAxOA==">
	<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
	</script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->
		
<!--		<div class="comment-section">
  
  


</div> -->
	</div>

	

</div>


		<footer>
			


<p>
  由 <a href="https://hexo.io">hexo</a> 强力驱动 |&nbsp;&nbsp;本站总点击 <span id="busuanzi_value_site_pv"></span> 次
&nbsp;&nbsp;|&nbsp;&nbsp;您是第 <span id="busuanzi_value_site_uv"></span> 位访客
</p>
<p>
  &copy; 2025 <a href="https://t1ger.github.io"> t1ger </a>
  &nbsp;&nbsp;|&nbsp;&nbsp;<span><a href="/baidusitemap.xml">百度网站地图</a></span>

</p>

<script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<script>
(function(){
    var bp = document.createElement('script');
    bp.src = '//push.zhanzhang.baidu.com/push.js';
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

<a id="gotop" href="#" title="back to top"><i class="mdi-hardware-keyboard-arrow-up"></i></a>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-99921399-1', 'auto');
  ga('send', 'pageview');

</script>
		</footer>
	  </div>

		<!-- <script src="/libs/bs/js/bootstrap.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap/3.3.4/js/bootstrap.min.js"></script>
		<script>(typeof $().modal == 'function')|| document.write('<script src="/libs/bs/js/bootstrap.min.js" type="text/javascript"><\/script>')</script>

		<!-- material design -->
		<!-- <script src="/libs/bs-material/js/ripples.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap-material/0.3.0/js/ripples.min.js"></script>
		<!-- <script src="/libs/bs-material/js/material.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap-material/0.3.0/js/material.min.js"></script>
		<!-- toc -->
		<!-- <script src="/libs/tocify/jquery-ui.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/jqueryui/1.10.4/jquery-ui.min.js"></script>
		<script src="/libs/tocify/jquery.tocify.custom.js"></script>

		<script src="/js/main.js"></script>

	</body>
</html>
